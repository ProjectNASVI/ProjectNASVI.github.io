"use strict";(self.webpackChunknasvi=self.webpackChunknasvi||[]).push([[33],{2723:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>d});var s=i(4848),t=i(8453);const r={sidebar_position:2},o=void 0,a={id:"Documentation/Documentation-v2",title:"Documentation-v2",description:"---",source:"@site/docs/Documentation/Documentation-v2.md",sourceDirName:"Documentation",slug:"/Documentation/Documentation-v2",permalink:"/docs/Documentation/Documentation-v2",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Documentation-v1",permalink:"/docs/Documentation/Documentation-v1"},next:{title:"Expanded-Documentation",permalink:"/docs/category/expanded-documentation"}},l={},d=[{value:"Project Documentation: Project NASVI",id:"project-documentation-project-nasvi",level:2},{value:"Smart Navigation Assistance System for the Visually Impaired",id:"smart-navigation-assistance-system-for-the-visually-impaired",level:3},{value:"1. Introduction",id:"1-introduction",level:3},{value:"2. Background",id:"2-background",level:3},{value:"3. Project Goals and Benefits",id:"3-project-goals-and-benefits",level:3},{value:"4. Hardware Components",id:"4-hardware-components",level:3},{value:"5. Software Components",id:"5-software-components",level:3},{value:"6. System Design and Architecture",id:"6-system-design-and-architecture",level:3},{value:"7. Implementation",id:"7-implementation",level:3},{value:"8. Testing and Calibration",id:"8-testing-and-calibration",level:3},{value:"9. Future Enhancements",id:"9-future-enhancements",level:3},{value:"10. Conclusion",id:"10-conclusion",level:3}];function c(e){const n={h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"project-documentation-project-nasvi",children:"Project Documentation: Project NASVI"}),"\n",(0,s.jsx)(n.h3,{id:"smart-navigation-assistance-system-for-the-visually-impaired",children:"Smart Navigation Assistance System for the Visually Impaired"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Project Overview"}),": Project NASVI is an advanced ",(0,s.jsx)(n.strong,{children:"Smart Navigation Assistance System"})," that helps visually impaired individuals navigate complex environments independently. Equipped with a ",(0,s.jsx)(n.strong,{children:"Raspberry Pi"}),", ",(0,s.jsx)(n.strong,{children:"camera"}),", and ",(0,s.jsx)(n.strong,{children:"earpiece"}),", NASVI utilizes real-time ",(0,s.jsx)(n.strong,{children:"object detection"}),", ",(0,s.jsx)(n.strong,{children:"path recognition"}),", and ",(0,s.jsx)(n.strong,{children:"audio feedback"})," to provide immediate information about the user\u2019s surroundings. This enhances the user\u2019s situational awareness and promotes mobility without traditional aids."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Motivation"}),": This project was inspired by observing the daily challenges that blind individuals face, including the limited reach of white canes and the high cost of guide dogs. NASVI provides a ",(0,s.jsx)(n.strong,{children:"cost-effective, real-time, and hands-free"})," alternative, enabling users to access information about their surroundings and enhancing their safety and independence."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose and Objectives"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enable Real-Time Navigation"}),": The system provides immediate feedback, helping users detect nearby objects, paths, and obstacles."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhance Safety"}),": Alerts users to potential dangers in the environment, such as vehicles or unexpected objects."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Promote Independence"}),": By providing verbal guidance, NASVI minimizes the need for external assistance and promotes self-reliant navigation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"2-background",children:"2. Background"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Global and Indian Blindness Statistics"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Global Statistics"}),": According to the ",(0,s.jsx)(n.strong,{children:"World Health Organization (WHO)"}),", about ",(0,s.jsx)(n.strong,{children:"2.2 billion people"})," globally suffer from some form of vision impairment or blindness. Out of these, nearly ",(0,s.jsx)(n.strong,{children:"1 billion cases could have been prevented"})," or treated."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"India\u2019s Statistics"}),": India has one of the highest populations of visually impaired individuals, with over ",(0,s.jsx)(n.strong,{children:"12 million blind people"}),". This represents about ",(0,s.jsx)(n.strong,{children:"20% of the world\u2019s blind population"}),". High rates of untreated cataracts, glaucoma, and uncorrected refractive errors contribute to this figure."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Limitations of Traditional Solutions"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"White Canes"}),": While canes are affordable, they have a limited detection range and primarily cover obstacles within arm\u2019s reach. They do not alert users to overhead objects (e.g., signboards, tree branches)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Guide Dogs"}),": Effective yet costly, guide dogs require extensive training, maintenance, and access to facilities that may not be available to everyone, especially in developing regions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dependency"}),": Both canes and guide dogs require continuous physical interaction or dependency on external assistance, which may not provide the user with a full range of environmental awareness."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"3-project-goals-and-benefits",children:"3. Project Goals and Benefits"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goals"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design a system capable of recognizing a variety of objects, obstacles, and pathways in real-time."}),"\n",(0,s.jsx)(n.li,{children:"Provide non-intrusive, continuous audio feedback for hands-free navigation."}),"\n",(0,s.jsx)(n.li,{children:"Enhance the user\u2019s confidence and self-sufficiency in moving through different environments."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benefits"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Independence"}),": By receiving real-time navigation cues, users can move independently and confidently without the need for a guide or assistant."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhanced Safety"}),": Real-time detection helps prevent collisions, fall hazards, and other dangers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Affordability"}),": Using widely available and affordable components (Raspberry Pi, camera, earpiece), the system can be replicated and distributed at a lower cost than guide dogs or more sophisticated assistance technologies."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"4-hardware-components",children:"4. Hardware Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Raspberry Pi 4 (2GB or Higher)"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": The core computing unit, responsible for running object detection models, processing camera inputs, and generating audio feedback."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Cost-effective, energy-efficient, and capable of handling AI processing tasks for real-time applications."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Specification"}),": 2GB RAM is the minimum recommended for this project, with higher memory configurations (4GB or 8GB) potentially enhancing performance."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Raspberry Pi Camera Module"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Function"}),": Captures live video feeds from the user\u2019s surroundings."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Placement"}),": Positioned to provide a wide field of view to capture nearby obstacles and pathways."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Specifications"}),": Resolution between 5-12 megapixels recommended to capture clear, detailed visuals for object detection models."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Earpiece or Headphones"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": Provides audio feedback in real-time, delivering navigation instructions and alerts."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Functionality"}),": Ensures the user receives instructions privately and without disturbing others."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Portable Power Source"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Battery Pack"}),": Provides the Raspberry Pi and camera module with a reliable, portable power supply, ensuring several hours of continuous use."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Specifications"}),": A power bank with 5V output and at least 10,000mAh capacity for extended operation."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"5-software-components",children:"5. Software Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Operating System (Raspberry Pi OS)"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overview"}),": A Linux-based, lightweight OS optimized for Raspberry Pi\u2019s hardware. Supports essential libraries for computer vision and object detection."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Setup"}),": Install and configure to enable efficient resource allocation, with minimal background processes to maximize performance for real-time detection tasks."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Detection Algorithms (YOLO or MobileNet)"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": Lightweight, pre-trained models like YOLO or MobileNet provide high accuracy and efficiency, detecting multiple object types in real-time."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Purpose"}),": Recognizes common obstacles and objects such as vehicles, pedestrians, and signposts, helping users navigate safely."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text-to-Speech (TTS) Engine (pyttsx3 or gTTS)"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Engine"}),": Converts object detection results into spoken instructions, providing the user with verbal guidance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantages"}),": Supports offline functionality (pyttsx3) or online API-based options (gTTS) for multilingual capabilities."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Programming Language (Python)"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Choice of Language"}),": Python is chosen due to its extensive libraries for computer vision, text-to-speech, and compatibility with Raspberry Pi."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Libraries"}),": Includes TensorFlow, OpenCV, and Pyttsx3 for streamlined image processing and audio output."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"6-system-design-and-architecture",children:"6. System Design and Architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Detection Module"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Capture"}),": The Raspberry Pi Camera captures continuous video frames of the user\u2019s surroundings."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Recognition"}),": Each frame is processed by a pre-trained deep learning model that identifies objects, classifying them into categories like vehicles, obstacles, or open pathways."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback System"}),": Generates audio feedback based on detected objects (e.g., \u201cObstacle to your left,\u201d \u201cClear path ahead\u201d)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Path Detection and Obstacle Avoidance"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Edge Detection"}),": Uses edge-detection algorithms to identify paths and safe walking zones within the frame."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Obstacle Distance Measurement"}),": Calculates the distance to identified obstacles and informs the user to help them avoid potential hazards."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Feedback System"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Conversion"}),": Converts identified object data into simple spoken instructions, guiding the user verbally."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Continuous Output"}),": The earpiece delivers ongoing navigation feedback in real-time, enabling the user to stay aware of their surroundings without needing to pause."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"7-implementation",children:"7. Implementation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Setup Process"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Install the Raspberry Pi OS and required dependencies."}),"\n",(0,s.jsx)(n.li,{children:"Set up the camera module and run configuration tests to verify proper connection."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Coding and Configuration"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object Detection: Code the detection model to capture and process video frames, identifying objects and pathways."}),"\n",(0,s.jsx)(n.li,{children:"TTS Integration: Implement the text-to-speech library, converting object data into audio feedback for the user."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing and Optimization"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Run tests in different environments to assess performance."}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune object detection sensitivity and TTS output for clarity and response speed."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"8-testing-and-calibration",children:"8. Testing and Calibration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing Phases"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Controlled Environments"}),": Start by testing in safe, controlled spaces (indoor and outdoor) to assess object recognition and audio feedback accuracy."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-world Testing"}),": Gradually move to more complex settings, like public parks and streets, to evaluate navigation reliability."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calibration"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Fine-tune object detection and path recognition thresholds to reduce false positives and ensure faster processing."}),"\n",(0,s.jsx)(n.li,{children:"Adjust audio feedback timing to improve synchronization with real-time object detection."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"9-future-enhancements",children:"9. Future Enhancements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPS Integration"}),": Implement GPS for outdoor location tracking and geolocation-based navigation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Support"}),": Introduce multilingual audio feedback for broader accessibility."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhanced AI Models"}),": Train custom AI models that provide more accurate navigation feedback in crowded or complex environments."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"10-conclusion",children:"10. Conclusion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Summary"}),": Project NASVI provides a novel solution to the navigation challenges faced by visually impaired individuals. By using affordable hardware and intelligent software, NASVI delivers real-time audio feedback to help users avoid obstacles and navigate paths independently."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Impact"}),": With further development, this project has the potential to make a meaningful difference in the lives of visually impaired individuals, particularly in regions where access to assistance tools is limited."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:"This detailed documentation covers every essential aspect of Project NASVI, offering a clear guide on objectives, setup, functionality, and future potential."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);